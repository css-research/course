# Overview

Due before class May 8th.

# Fork the `hw01` repository

Go [here](https://github.com/css-research/hw01) to fork the repo for homework 01.

# Classification using neural networks

1. Set your random seed to `1234`
    * R
    
        ```r
        set.seed(1234)
        ```
    * Python
    
        ```python
        import random
        random.seed(1234)
        ```
1. Load the [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset
    * [The dataset is built into Keras](https://keras.io/datasets/#fashion-mnist-database-of-fashion-articles)
    * Preprocess the data by converting the data to a 2D tensor with individual values between 0 and 1
    * Randomly split the training data into 50,000 training observations and 10,000 validation observations
1. Implement a series of neural network models
    1. Initial test
        * 5 dense, fully-connected layers
        * `relu` activation except for the last layer (use `softmax`)
        * Initialize with 512 hidden units apiece (except for the last layer)
        * Use `rmsprop` optimizer
        * Use categorical crossentropy for loss function
        * Track validation set accuracy during training process
        * Train with `batch_size = 512` and 200 epochs
        * Plot the validation set accuracy and loss over the epochs
        * Identify the epoch where the model's performance degrades based on the validation set
    1. Implement dropout
        * Implement layer dropout after each layer from model 1 (except the last)
        * Use a dropout rate of `0.5`
        * Estimate the model, and graphically compare the validation loss across epochs to the initial model. How does this new model perform relative to the old model?
    1. Weight regularization
        * Reestimate the initial model with L1 weight regularization on each layer (except the final layer) with a `0.001` penalty for each weight coefficient
        * Reestimate the initial model with L2 weight regularization on each layer (except the final layer) with a `0.001` penalty for each weight coefficient
        * Plot the validation loss for the initial model vs. the dropout vs. the L1 regularized model vs. the L2 regularized model - which model appears to perform the best?
    1. Evaluate a minimum of 10 alternative models, using different hyperparameter options (e.g. number of layers, number of hidden units, batch size, epochs, dropout ratios, weight regularization penalties)
1. Final model
    * Select the best model from the ones you have estimated so far - this should have the lowest validation loss score at any potential epoch
    * Reestimate that model using all of the training data (no validation set) with the same hyperparameter values
    * Calcuate the test set loss and accuracy. How well does the model generalize?
    * Why do you think this model performed better/worse than your other model configurations? What about the design of the network led to improved performace?

# Estimating your deep learning models

You can use any computing platform to complete this assignment. However the best performance will come from using a GPU-enabled computer. I recommend using an Amazon AWS EC2 instance as outlined in the textbook. You can use an alternative platform (e.g. Google Cloud, RCC, your laptop), but I will not offer technical support for any other environment.

## Setup Amazon AWS EC2 instance

> You can refer to Appendix B in Deep Learning for Python/R for more detailed instructions. It may be easier to use the AMI I link to in the instructions below.

* Create an account on [Amazon AWS](https://aws.amazon.com/)
* Request an increase for `p2.xlarge` instances via [these instructions](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html#request-increase). **This will take several hours to process before you can proceed to the next step.**
* [Create a new instance using this AMI with a `p2.xlarge` instance](https://aws.amazon.com/marketplace/pp/B0785SXYB2)
* Download your key and store it in a dedicated folder
* Open the terminal and `cd` to the folder with your key
* SSH into the instance once active
    * `ssh -i "<key.pem>" ubuntu@<ec2>`

### For Python and Jupyter Notebook

* You need to first install the most recent version of TensorFlow and Keras

    ```python
    sudo pip3 install tensorflow-gpu
    sudo pip3 install keras
    ```
* Launch `jupyter notebook`
    * Note the url/token displayed in the terminal
* Log into Jupyter Notebook server
    * Open a new local terminal
    * `cd` to the folder with your key
    * SSH a new connection
    
        ```bash
        ssh -N -L 8888:localhost:8888 -i <key.pem> ubuntu@<ec2>
        ```
    * Use your web browser to go to the link provided in the previous terminal window. This link will work as long as the Jupyter Notebook server is running and your SSH connection to `localhost:8888` is active.

### For R and RStudio Server

1. Ensure your instance is started in your EC2 dashboard
1. Visit `http://ec2:8787`, replacing `ec2` with the public DNS provided by Amazon (it should look something like `http://ec2-34-423-564-213.compute-1.amazonaws.com`)
1. Log in with the username `rstudio-user` and the instance_id of the instance as the password
1. Once logged in, please set a new password for this user:
    * Click the "Tools" menu dropdown
    * select "shell"
    * type "passwd" into the shell
    * enter the current password (instance_id) and the new password twice, hitting the enter key after each entry.
1. You should be good to go from here. You may need to update `keras` and `tensorflow`.

### Warnings

* You are charged \$.90 per hour whenever your instance is running. When you are finished:
    * Stop the instance in the AWS console to avoid overcharging
    * Delete all associated EBS volumes where your data is stored
* You can sign up for an AWS Educate account which gives you free credits to apply towards AWS usage
* Make sure to store all your work in a Git version controlled repository. **Make sure to clone your course project repo, store all your files in there, and commit/push back to GitHub before you stop the instance.**

# Submission format

* Submit your solution to each part as a separate Jupyter Notebook (if Python) or R Markdown document (if R)
